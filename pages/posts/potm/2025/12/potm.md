---
date: 2026-01-13
categories:
- Papers of the Month
title: 'December Papers: MoE, Fact-storing and Byteifying Language Models'
merge_potm: true
---

Despite the holiday season and the busy NeurIPS period, December closed the year with set of insightful papers. Our team reviewed the following three papers:

<!-- SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations -->
- First up, [SonicMoE](#sonicmoe-accelerating-moe-with-io-and-tile-aware-optimizations) tackles issues of fine-grained and sparse MoEs using hardware-aware optimizations to restore efficiency.

<!-- Constructing Efficient Fact-Storing MLPs for Transformers -->
- Next, [Constructing Efficient Fact-Storing MLPs for Transformers](#constructing-efficient-fact-storing-mlps-for-transformers) shows how MLP layers can be explicitly constructed as keyâ€“value stores to achieve high facts-per-parameter efficiency. 

<!-- Bolmo: Byteifying the Next Generation of Language Models -->
- Finally, [Bolmo](#bolmo-byteifying-the-next-generation-of-language-models) presents a method for "byteifying" existing subword-level language models that improves character-level understanding while achieving comparable performance to subword-level models.
