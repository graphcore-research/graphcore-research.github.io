---
date: 2025-10-07
categories:
- Papers of the Month
title: 'September Papers: The L in ML Stands for LLMs'
merge_potm: true
---

For September, the research team reviewed a whopping 22 papers! Needless to say, competition was fierce, and only four made the final cut for this month’s edition, which is LLM-themed:  

- [FlowRL](#flowrl-matching-reward-distributions-for-llm-reasoning) uses GFlowNets to train LLMs on full reward distributions, promoting diverse reasoning paths instead of just reward maximization.  
- [Soft Tokens, Hard Truths](#soft-tokens-hard-truths) proposes using continuous “soft” tokens with injected noise to enable reinforcement learning fine-tuning of LLM reasoning.  
- [Set Block Decoding](#set-block-decoding-is-a-language-model-inference-accelerator) accelerates LLM inference by generating multiple tokens in parallel using non-causal attention and iterative entropy-based sampling.  
- [Metacognitive Reuse](#metacognitive-reuse-turning-recurring-llm-reasoning-into-concise-behaviors) enables LLMs to extract and reuse concise reasoning “behaviors” to improve efficiency and reduce repeated computation.  
