---
title: "October: Elastic Inference and Shrinking Formats"
date: 2023-11-01T15:34:30-04:00
categories:
  - blog
tags:
  - papers-of-the-month
  - inference
  - formats
---

An intro blurb outlining the papers we're going to cover...

{% assign image_dir = site.url | append: site.baseurl | append: "/assets/images/posts/2023-11/" %}

## [BitNet: Scaling 1-bit Transformers for LLMs](https://arxiv.org/abs/2310.11453){:target="_blank"}

_Hongyu Wang, Shuming Ma, et al. (Microsoft Research, University of Chinese Academy of Sciences, Tsinghua University)_

### The key idea

The article is presenting a method for training LLMs using 1-bit linear layers. In this architecture, weights are stored in 1 bit format, and input activations are quantized on-the-fly after normalization. This work is only considering quantization aware training (QAT), not post-training quantization (PTQ).

![image]({{image_dir}}/bitnet/figure_1.png)

### Their method

In terms of implementation, the approach is following previous lower precision papers, with an ad-hoc quantized linear layer. The main difference is the addition of (sharded) layer norm before quantization.

![image]({{image_dir}}/bitnet/figure_2.png)

### Results

Results are promising, showing that despite the reduction in terms of accuracy, it could be an order of magnitude more efficient that FP16 and FP8 on hardware. Note nevertheless that 4-bits baselines look a bit weak, and additional experiments on SOTA models like LLama would be useful to confirm the results.

![image]({{image_dir}}/bitnet/table_3.png)

