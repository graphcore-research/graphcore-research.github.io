---
title: "Sparsity, Speculative Sampling and Schnitzel - our ICML 2024 roundup"
date: 2024-08-08T01:00:00-00:00
header:
    teaser: /assets/images/posts/2024-08/icml/vienna.png
    image: /assets/images/posts/2024-08/icml/vienna.png
    og_image: /assets/images/posts/2024-08/icml/vienna.png

layout: single
category: "posts"
toc: true
toc_sticky: true
toc_label: "ICML 2024 Roundup"
toc_icon: "book"
image_dir: "/assets/images/posts/2024-08/icml/"
author: Luke Hudlass-Galley
---

The 2024 International Conference on Machine Learning (ICML) was held last month in Vienna, Austria. As one of the "big three" AI conferences, alongside ICLR and NeurIPS, it attracted thousands of AI researchers and practitioners from around the globe. Attendees flocked to the historic city to present their work and discover the latest advancements in this rapidly evolving field. In this post, we highlight some of the topics and papers that piqued our interest.

---

### All things LLMs

Perhaps unsurprisingly, LLMs continue to be the talk of the town - a quick search at the ICML schedule shows there were over 300 posters presented with either "LLM" or "language model" in their title.

A number of these LLM papers aim to address the inefficiencies of pre-training and fine-tuning these models. Tackling inefficient fine-tuning, [RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation](https://icml.cc/virtual/2024/poster/34527) introduce a parameter-efficient fine-tuning (PEFT) strategy that jointly trains low-rank and highly-sparse components, outperforming other PEFT approaches such as LoRA. [Exploring the Benefit of Activation Sparsity in Pre-training](https://icml.cc/virtual/2024/poster/34332) propose incorporating sparse computation within FFN layers in the later stages of pre-training, achieving up to 1.44x pre-training speed-up and improved inference efficiency. [Accelerating Transformer Pre-training with 2:4 Sparsity](https://icml.cc/virtual/2024/poster/33254) considers semi-structured pruning as a practical hardware-aware technique to accelerate pre-training.

<img class="constrained_img" src="{{ page.image_dir | append: 'attention-training.png' | relative_url }}" alt="Figure 1: Robust Adaptation (RoSA) scheme applied to a single fully-connected later (left); illustration of Switchable Sparse-Dense Learning (right)."/>

Inference was another topic receiving a lot of attention (forgive the pun), due to how expensive autoregressive generation can be. Several works build upon [speculative sampling](https://arxiv.org/pdf/2302.01318) to improve inference. [Accelerated Speculative Sampling Based on Tree Monte Carlo](https://icml.cc/virtual/2024/poster/32890) relates the technique to the concept of _maximum coupling_, a strategy in Monte Carlo simulations. By reframing the distribution from the token space to a tree space, this work obtains a higher average number of accepted tokens compared to conventional speculative sampling, and thus can accelerate performance. [EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty](https://icml.cc/virtual/2024/poster/35153) suggest a draft model which comprises of a single (trainable) decoder layer operating on the feature level of the target model. 

<img class="constrained_img" src="{{ page.image_dir | append: 'eagle.png' | relative_url }}" alt="Figure 2: EAGLE performs autoregressive generation at the feature level instead of the token level during the drafting stage."/>

Addressing the cost of large KV caches was another popular angle being investigated in the LLM inference space. Works such as [Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference](https://icml.cc/virtual/2024/poster/32813), [LoCoCo: Dropping In Convolutions for Long Context Compression](https://icml.cc/virtual/2024/poster/34202), [Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference](https://icml.cc/virtual/2024/poster/32874) and [CaM: Cache Merging for Memory-efficient LLMs Inference](https://icml.cc/virtual/2024/poster/34310) deviate from eviction methods by storing less influential token data in a compressed state - LESS adopts a low rank approximation of this state, whereas LoCoCo and Dynamic Memory Compression (DMC) utilise a weighted combination of tokens (LoCoCo uses 1D convolutional kernels that dynamically calculate mixing weights for each KV cache slot, whereas DMC introduces a fine-tuning method to determine whether a token should extend the KV cache or be accumulated into a weighted average). CaM merges to-be-evicted tokens with higher attention scores into subsequence tokens. 

Collectively, the above papers support the notion that eviction-based approaches are rather limited, and not robust to many long-sequence tasks. At Graphcore Research we have observed the same limitations, which prompted our work on [SparQ Attention](https://arxiv.org/abs/2312.04985), which was also [presented at ICML](https://icml.cc/virtual/2024/poster/34162).

<img class="constrained_img" src="{{ page.image_dir | append: 'sparq-poster.png' | relative_url }}" alt="Forgive the SparQ plug (get it, SparQ plug? Nevermind, I'll see myself out.)"/>

Other papers investigating the KV cache include [KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache](https://icml.cc/virtual/2024/poster/34318), which proposes a 2-bit compression scheme for the KV cache, motivated by a study of the KV cache element distribution in various LLMs; [InfLLM: Training-Free Long-Context Extrapolation for LLMs with an Efficient Context Memory](https://openreview.net/forum?id=i5SEw4guQK), which uses _representative tokens_ as a way of compressing blocks of evicted tokens.









