---
title: "Sparsity, Speculative Sampling and Schnitzel - our ICML 2024 roundup"
date: 2024-08-08T01:00:00-00:00
header:
    teaser: /assets/images/posts/2024-08/icml/vienna.png
    image: /assets/images/posts/2024-08/icml/vienna.png
    og_image: /assets/images/posts/2024-08/icml/vienna.png

layout: single
category: "posts"
toc: true
toc_sticky: true
toc_label: "ICML 2024 Roundup"
toc_icon: "book"
image_dir: "/assets/images/posts/2024-08/icml/"
author: Luke Hudlass-Galley
---

The 2024 International Conference on Machine Learning (ICML) was held last month in Vienna, Austria. As one of the "big three" AI conferences, alongside ICLR and NeurIPS, it attracted thousands of AI researchers and practitioners from around the globe. Attendees flocked to the historic city to present their work and discover the latest advancements in this rapidly evolving field. In this post, we highlight some of the topics and papers that piqued our interest.

---

### All things LLMs

Perhaps unsurprisingly, LLMs continue to be the talk of the town - a quick search at the ICML schedule shows there were over 300 posters presented with either "LLM" or "language model" in their title.

A number of these LLM papers aim to address the inefficiencies of pre-training and fine-tuning these models. Tackling inefficient fine-tuning, [RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation](https://icml.cc/virtual/2024/poster/34527) introduce a parameter-efficient fine-tuning (PEFT) strategy that jointly trains low-rank and highly-sparse components, outperforming other PEFT approaches such as LoRA. [Exploring the Benefit of Activation Sparsity in Pre-training](https://icml.cc/virtual/2024/poster/34332) propose incorporating sparse computation within FFN layers in the later stages of pre-training, achieving up to 1.44x pre-training speed-up and improved inference efficiency. [Accelerating Transformer Pre-training with 2:4 Sparsity](https://icml.cc/virtual/2024/poster/33254) considers semi-structured pruning as a practical hardware-aware technique to accelerate pre-training.

Inference was another topic receiving a lot of attention (forgive the pun), due to how expensive autoregressive generation can be. Several works build upon [speculative sampling](https://arxiv.org/pdf/2302.01318) to improve inference. [Accelerated Speculative Sampling Based on Tree Monte Carlo](https://icml.cc/virtual/2024/poster/32890) relates the technique to the concept of _maximum coupling_, a strategy in Monte Carlo simulations. By reframing the distribution from the token space to a tree space, this work obtains a higher average number of accepted tokens compared to conventional speculative sampling, and thus can accelerate performance. [EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty](https://icml.cc/virtual/2024/poster/35153) suggest a draft model which is a single (trainable) decoder layer operating on the feature level of the target model. 












