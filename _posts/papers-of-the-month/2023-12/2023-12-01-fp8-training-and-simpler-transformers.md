---
title: "December Papers: FP8 Training & Simpler Transformers"
header.teaser: /assets/images/posts/2023-12/potm/chipnemo/fig1.png

date: 2023-12-01T01:34:30-04:00
potm_year: 2023
potm_month: 12

layout: paper-summaries-layout
category: "papers-of-the-month"
toc: true
toc_label: "Papers"
toc_icon: "book"
---

The last month saw impressive developments in the space of efficient transformers
and applied ML, from materials discovery to chip design.

Researchers at Microsoft showed that FP8 could be used in parts of the LLM training
process that until now had been kept in higher-precision, and work from ETH Zurich
suggested a simplified way of designing transformer-like models.

In terms of applications, DeepMind have impressive results showing that GNNs can be used
in the discovery of new inorganic crystals â€” a key building block of many modern
technologies. Nvidia have also trained up a model to assist their
engineers on chip design. This is a neat feedback loop: their chip design has
facilitated better LLMs, and now their LLMs could facilitate better chip design.
How useful this will be in practice remains to be seen.

{% include paper-summaries.md %}
