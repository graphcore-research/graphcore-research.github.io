---
title: "SOAP: Improving and Stabilizing Shampoo using Adam"
paper_authors: "Nikhil Vyas, Depen Morwani, et al."
orgs: "Harvard University"
paper_link: "https://arxiv.org/abs/2409.11321"
tags:
    - optimization
potm_year: 2024
potm_month: 9
paper_order: 1  # Editor will decide
image_dir: "/assets/images/posts/2024-09/potm/soap/"
review_author:
    name: "Douglas Orr"
    link: "https://www.linkedin.com/in/douglas-orr-b3335984"
hidden: true
---

### The key idea

It turns out that Shampoo (explained below), with some minor tweaks, is equivalent to running Adafactor in Shampoo's eigenspace. Since Adafactor is a rank=1 variant of Adam, the proposed method SOAP runs Adam in Shampoo's eigenspace instead.

<img class="constrained_img_large" src="{{ page.image_dir | append: 'headline.png' | relative_url }}" alt="SOAP performance versus Adam and Shampoo, showing good step-efficiency (due to Adam) and time-efficiency (due to periodic preconditioning). Less frequent preconditioning hurts Shampoo more than SOAP.">

### Background

[Shampoo](https://arxiv.org/abs/1802.09568) for matrices looks like this:

<div>$$\begin{aligned}
L_t &= L_{t-1} + G_t G_t^{\top} \\
R_t &= R_{t-1} + G_t^{\top} G_t \\
W_t &= W_{t-1} - \eta \cdot L_t^{-1/4} G_t R_t^{-1/4}
\end{aligned}$$</div>

Where $W \in \Re^{m \times n}$ is a weight matrix, $L\in \Re^{m \times m}$, $R\in \Re^{n \times n}$ are "preconditioners", behaving a bit like optimizer state and $G$ is the minibatch gradient.

A slightly different variant is considered here: idealized Shampoo with power $1/2$,

<div>$$\begin{aligned}
L &= \mathbb{E}(G G^{\top}) \\
R &= \mathbb{E}(G^{\top} G) \\
W_t &= W_{t-1} - \eta \cdot L^{-1/2} G_t R^{-1/2} \,/\, \mathrm{tr}(L)
\end{aligned}$$</div>

The authors show that the last line is equivalent to idealized Adafactor in the _Shampoo eigenspace_:

<div>$$\begin{aligned}
Q_L &= \mathrm{Eigenvectors}(L) \\
Q_R &= \mathrm{Eigenvectors}(R) \\
G^{\prime} &= Q_L^{\top} G Q_R \\
W_t &= W_{t-1} - \eta \cdot Q_L^{\top} \mathrm{Adafactor}(G^{\prime}) Q_R
\end{aligned}$$</div>

### Their method

Based on this link between Shampoo and Adafactor, the authors propose SOAP, which runs full Adam in the Shampoo eigenspace and increases efficiency by only updating the eigenvectors periodically (e.g. every 10 steps).

<img class="constrained_img_large" src="{{ page.image_dir | append: 'algorithm.png' | relative_url }}" alt="SOAP algorithm panel, running Adam in the Shampoo eigenspace.">

The running state of this technique includes $L$, $R$, $Q_L$, $Q_R$, $M$ (in the weight space) and $V$ (in the Shampoo eigenspace). For large projections, such as the final projection layer in an LLM, the corresponding $Q_L$ or $Q_R$ can be fixed to identity. If both are fixed, SOAP reproduces Adam.

### Results

Results on language modelling (see figure above) show good step-efficiency of SOAP since it is based on Adam rather than Adafactor, and time-efficiency since the eigenvectors can be periodically updated without substantially harming performance. Like Shampoo, the extra optimization cost can be reduced by using a large batch size.

Stepping back for a moment, I'm excited about this progress using Shampoo variants and am eager to see experiments over long training runs of LLMs. Hopefully, then, we'll see plenty more shower-related puns on arXiv over the next year!
