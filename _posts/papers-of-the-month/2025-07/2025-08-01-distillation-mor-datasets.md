---
title: "July Papers: Subliminal Learning, Mixture of Recursions and Dataset Curation"
header:
    teaser: /assets/images/posts/2025-07/potm/twitter_card.png
    image: /assets/images/posts/2025-07/potm/twitter_card.png
    og_image: /assets/images/posts/2025-07/potm/twitter_card.png

date: 2025-08-01T01:00:00-00:00
potm_year: 2025
potm_month: 7

layout: paper-summaries-layout
category: "papers-of-the-month"
toc: true
toc_sticky: true
toc_label: "Papers"
toc_icon: "book"
author.twitter: "GCResearchTeam"
---

As July brought tennis at Wimbledon, so too did the ML world serve up a volley of research. This month, we took an eagle-eyed approach—or, perhaps, *Hawk Eye*d approach—to three papers.

In our first paper, [Subliminal Learning](#subliminal-learning-language-models-transmit-behavioral-traits-via-hidden-signals-in-data) addresses the question, "Can we control or filter the distillation training data so that a student learns desirable properties but avoids picking up undesirable traits?" The authors conclude that the student learns *all* the teacher's traits, whether they're desirable or not!

Next, [Mixture of Recursions](#mixture-of-recursions-learning-dynamic-recursive-depths-for-adaptive-token-level-computation) brings a twist to token-level computation: instead of fixed-depth processing, the model learns to recurse adaptively, allocating compute per token dynamically and efficiently—like a rally whose length depends on the importance of the point.

Last up is [DataRater](#datarater-meta-learned-dataset-curation), where the problem of dataset quality is addressed. A 'rater' is meta-learned to curate training data without manual filtering—an ace for data-centric AI.

*We hope you enjoy this month's papers as much as we did! If you have thoughts or questions, please reach out to us at [@GCResearchTeam](https://x.com/GCResearchTeam).*

---

{% include paper-summaries.md %}
