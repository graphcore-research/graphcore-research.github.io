---
title: "M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models"
paper_authors: "Junxiong Wang et al."
orgs: "TogetherAI, Cornell University, University of Geneva, Princeton University"
paper_link: "https://arxiv.org/abs/2504.10449"
tags:
    - LLMs
    - mamba
    - reasoning
potm_year: 2025
potm_month: 4
paper_order: 3
image_dir: "/assets/images/posts/2025-04/potm/mamba-reasoning/"
review_author:
    name: "Kheeran K. Naidu"
    link: "hhttps://kheerannaidu.com/"
hidden: true
---

### The key idea
Reasoning models, which are predominantly transformer-based, have recently benefited from longer chain-of-thought reasoning, that is, a longer context. 
These transformer-based models, however, suffer from a quadratic increase in computational complexity with respect to context length. 
The authors tackle this problem using a novel hybrid reasoning model (M1) based on Mamba, which has been shown to be computationaly more efficient than transformers [Gu and Dao, COLM 2024]. In doing so, they give the first hybrid Mamba-based reasoning model whose performance on math reasoning tasks outperforms transformer-based reasoning models.

### Their method
The authors adapt the multistep distilation approach proposed in [Wang et al., NeurIPS 2024] to design their 3B parameter M1 model:
1. *Distillation.* They distil from Llama-3.2-3B into a hybrid Mamba model by adapting the corresponding framework in [Wang et al., NeurIPS 2024]. See Figure 1 for the procedure.

    <img src="{{ page.image_dir | append: 'fig1.png' | relative_url }}" alt="An algorithm detailing the procedure used to distil a pretrained Llama-3.2-3B transformer model.">
    <figcaption><strong>Figure 1.</strong> An outline of the MambaInLlama distilation procedure used in the M1 model, which is adapted from [Wang et al., NeurIPS 2024].</figcaption>

2. *Supervised Finetuning.* They finetune the resulting model from step 1 in two ways. First, on the large set of math problems OpenMathInstruct-2. Then, on math problems and solutions generated by reasoning models, which includes OpenR1-Math-220k, OpenThoughts-114k-math, and ServiceNow-AI/R1-Distill.

3. *Reinforcement Learning.* They subsequently enhance performance using an altered Group Relative Policy Optimization (GRPO) strategy. In particular, they remove the KL term that penalises policies that are different to the original, and they add an entropy term to encourage diversity in the policy. The altered GPRO loss they use is

    $$L_\text{GPRO}(\Theta) = \mathbb{E}\left[\frac{\pi_{\Theta}(a \mid s)}{\pi_{\Theta_\text{old}}(a \mid s)} \cdot \hat{A}(s, a)\right] + \eta \cdot H(\pi_\Theta).$$ 

### Results
The authors evaluate the performance of their M1-3B model by reasoning (Table 1), inference speed (Figure 2), and test-time scaling (Figure 3):

1. *Reasoning.* M1-3B outperforms all non-reasoning language models and the Qwen2.5-Math-7B-Instruct reasoning model in all benchmarks. It slightly underperforms against the DeepSeek-R1-Distill-Qwen-1.5B reasoning model in almost all benchmarks. 

    <img src="{{ page.image_dir | append: 'tab1.png' | relative_url }}" alt="Reasoning performance results for the hybrid Mamba-based M1-3B model and various transformer-based models.">
    <figcaption><strong>Table 1.</strong>Reasoning performance results. Pass@1 refers to a percentage mark using a single sample per question and Maj@32 refers to a majority vote on 32 samples per question. </figcaption>

2. *Inference Speed.* As batch size and generation length are scaled up, M1-3B shows better speed ups compared to other models of similar and even smaller size, with upto a 3x speed up when compared to the Llama-3.2-3B model, which M1-3B is based on.

    <img src="{{ page.image_dir | append: 'fig2.png' | relative_url }}" alt="Inference speed results for the hybrid Mamba-based M1-3B model and various transformer-based models.">
    <figcaption><strong>Figure 2.</strong>Inference speed results when using prompt length 256 and decoding length 4096 (left) and when using batch size 128 (right).</figcaption>

3. *Test-Time Scaling.* Given a fixed compute-time budget, M1-3B ultimately outperforms DeepSeek-R1-Distill-Qwen-1.5B on math reasoning tasks when allowed to scale either the number of scoring samples or the length of the context/chain-of-thought.

    <img src="{{ page.image_dir | append: 'fig3.png' | relative_url }}" alt="Inference speed results for the hybrid Mamba-based M1-3B model and various transformer-based models.">
    <figcaption><strong>Figure 3.</strong>Results for test-time scaling with a fixed compute time budget (x-axis) when allowed to vary number of samples (left) and length of context (right).</figcaption>


### Takeaways
The M1-3B model shows that it is not necessary to rely solely on transformers for designing reasoning models. This hybrid model built on Mamba obtains improved computational efficiency and performance over even the larger Qwen2.5-Math-7B-Instruct reasoning model. However, for the smaller DeepSeek-R1-Distill-Qwen-1.5B model, it is only able to gain an improved reasoning performance by taking advantage of the more efficient inference time, that is, allowing for 'more reasoning' in a fixed amount of time.

...
