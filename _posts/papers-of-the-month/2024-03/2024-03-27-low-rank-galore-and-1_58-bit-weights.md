---
title: "March Papers: Low-Rank Galore & 1.58-Bit Weights"
header:
    teaser: /assets/images/posts/2024-03/potm/twitter_card.png
    image: /assets/images/posts/2024-03/potm/twitter_card.png
    og_image: /assets/images/posts/2024-03/potm/twitter_card.png

date: 2024-03-30T01:00:00-00:00
potm_year: 2024
potm_month: 3

layout: paper-summaries-layout
category: "papers-of-the-month"
toc: true
toc_sticky: true
toc_label: "Papers"
toc_icon: "book"
author.twitter: "GCResearchTeam"
---

March was a fruitful month for AI research, with plenty of papers for us to choose from. A trend in the work we've selected is the pushing of previously published methods to their limits, in new creative ways.

We start with GaLore, similar to the popular [LoRA](https://arxiv.org/abs/2106.09685) method for cheap fine-tuning, but introducing a low-rank approximation to the _gradients_ instead of weights. It turns out this is particularly effective for pre-training.

Our second paper declares "The Era of 1-bit LLMs", showing that the previously published BitNet model can be tweaked for LLM training, such that weights can be rounded to either -1, 0 or 1. This is much stronger quantisation than most people thought possible. We also cover the DiPaCo paper, which demonstrates a method for scaling distributed MoE training, potentially to systems of such scale that they have to be distributed across datacentres.

Investigating a phenomenon that occurs as LLMs get larger, the Massive Activations paper brings valuable insight into why the numerics of LLMs tend to explode for certain tokens/hidden dimensions. We conclude with the G-Retriever paper, which provides a method for applying [retrieval augmented generation (RAG)](https://arxiv.org/abs/2005.11401) to textual graphs â€” something valuable in real-world applications where graph structures are commonplace.

{% include paper-summaries.md %}
