---
title: "DeepSeek-V3 & DeepSeek-R1 Technical Reports"
paper_authors: "DeepSeek-AI"
orgs: "DeepSeek-AI"
paper_link: "https://github.com/deepseek-ai/DeepSeek-V3"
tags:
    - efficient-inference
    - quantisation  # Use https://graphcore-research.github.io/tags/ as reference
potm_year: 2025
potm_month: 01
paper_order: 1  # Editor will decide
image_dir: "/assets/images/posts2025-01/potm/deepseek/"
review_author:
    name: "Alexandre Payot"
    link: "https://www.linkedin.com/in/alexandre-payot/"

hidden: true
---

With their V3 model, DeepSeek released an open-weight frontier model, trading benchmark for benchmark with Anthropic's October 2024 Claude-3.5-Sonnet.
The technical report gives a detailed account of the architecture of the model, the trade-offs that led to it, and the efficient implementation that enabled their final training run to take a headline grabbing $5.5M of GPU hours.

This success does not come out of nowhere! It is the logical result of the efficiency work and data curation work that DeepSeek has been publishing since their initial release of DeepSeek-LLM a year ago in January 2024. They have been vying with Meta's Llama family of models for the best open weight model for a year.

<!-- [200 words is a rough guide for the length of a summary.
Feel free to go a fair bit over or under if needs be.
The editor will fix any issues with images being rendered too wide/narrow etc.
See README for how to view locally if you wish to (not required. Contact CB if this
is broken for you.)] -->

### The key idea

DeepSeek's success comes from improvements along two complementary axes: FLOP efficiency of the model, and, curation of training data.

DeepSeek's efficiency is the result of excellent engineering and incremental improvements to their fine-grained mixture of expert (MoE) architecture. DeepSeek-V3 adds 4 innovations compared to its predecessors:

- Multi-token prediction (MTP),
- The _DualPipe_ Pipelining algorithm,
- Micro-scaled FP8 training,
- Loss-free load balancing on experts.


<!-- A few sentences outlining why the paper is interesting...

Add images where appropriate throughout. This section should always
have at least 1 key figure though.

*Please use high-res images (zoom in for those screenshots!)*

<img src="{{ page.image_dir | append: 'figure_1.png' | relative_url }}" alt="A specific and succinct sentence or two describing the figure (alt text). Valuable for seo and accessibility.">
<figcaption>Figure 1a. If the caption isn't included in the image, it should be added like so.</figcaption> -->

<!-- ### [optional] Background

If necessary, a short intro to background matierial needed to understand the method -->

### Their method

<!-- Latex can be included in the standard way, either inline: $R=\sum _{t=0}^{\infty }\gamma ^{t}r_{t}$

Or as a block:

<div>
$$
Q_{t+1}^{A}(s_{t},a_{t})=Q_{t}^{A}(s_{t},a_{t})+\alpha _{t}(s_{t},a_{t})\left(r_{t}+\gamma Q_{t}^{B}\left(s_{t+1},\mathop {\operatorname {arg~max} } _{a}Q_{t}^{A}(s_{t+1},a)\right)-Q_{t}^{A}(s_{t},a_{t})\right).
$$
</div>

Code can also be included in the standard way:

```
import popart

builder = popart.Builder()

# Build a simple graph
i1 = builder.addInputTensor(popart.TensorInfo("FLOAT", [1, 2, 32, 32]))
i2 = builder.addInputTensor(popart.TensorInfo("FLOAT", [1, 2, 32, 32]))

o = builder.aiOnnx.add([i1, i2])
``` -->

### Results

<!-- ...

### [optional] Takeaways

... -->
