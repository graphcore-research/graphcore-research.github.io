---
title: "DeepSeek-V3 & DeepSeek-R1 Technical Reports"
paper_authors: "DeepSeek-AI"
orgs: "DeepSeek-AI"
paper_link: "https://github.com/deepseek-ai/DeepSeek-V3"
tags:
    - efficient-inference
    - quantisation  # Use https://graphcore-research.github.io/tags/ as reference
potm_year: 2025
potm_month: 01
paper_order: 1  # Editor will decide
image_dir: "/assets/images/posts2025-01/potm/deepseek/"
review_author:
    name: "Alexandre Payot"
    link: "https://www.linkedin.com/in/alexandre-payot/"

hidden: true
---

With their V3 and R1 models, DeepSeek set a new state of the art open-weight models and trade benchmark to benchmark with the best models from Anthropic, Google and OpenAI.
The technical report gives a detailed account of the architecture of the model, the trade-offs that led to it, and the efficient implementation that enabled their final training run to take a headline grabbing $5.5M of GPU hours.

DeepSeek's double release is so packed with information that we don't have space to cover their contributions in detail in this issue of "Papers of the Month"!
Instead to strike a compromise, in this summary we give you short, clear descriptions of innovations and their impact on efficiency and model performance; and we will be back shortly with a deep-dive into DeepSeek's publications and how their releases compare to other labs'.

### The key idea

DeepSeek's success comes from improvements along three complementary axes: FLOP efficiency of the model, curation of training data, and, (re-)discovery of effective reinforcement learning for LLMs.

DeepSeek's efficiency is the result of excellent engineering and incremental improvements to their fine-grained mixture of expert (MoE) architecture. Their innovations are geared towards making that architecture run efficiently on their hardware and learn effectively.

<!-- R1-key idea shaped hole -->

While the reports are thorough, some elements are notable by their absence: the team does not share scaling laws (as done in the [Llama family of models report]()), and is unspecific about the dataset curation (as in the [Phi-4 technical report]()).

### Efficiency improvements

In this section we cover the architectural innovations and implementation optimisations that DeepSeek implemented to train and serve their model.

- name: FP8 micro-scaling
- description: Do matrix multiplication with FP8 tensors and scales associated with blocks ($N_c \times N_c$) and vectors ($N_c$) of those tensors (MXFP8). The "trick" is to accumulate inside blocks in FP8 and apply the scale and accumulate between blocks in FP32.
- Impact on efficiency: 2x FLOPs available, 1/2 communication, 1/2 memory usage (capacity and BW)
- Impact on model performance: Loss curves in Appendix A.1 show exact match.

- name: Multi-token Prediction
- description: A module trained with the main model to predict an extra token from the last transformer layer's output. It shares the embedding and the output layers with the main model, and adds 1 transformer block.
- Impact on efficiency: training - ~1/61 increase in compute - inference: predict 2 tokens instead of 1 85-90% of the time by using it as a speculative decoder.
- Impact on model performance: on average 2% improvement across benchmarks (extremes: -1%, +8%).

- name: DualPipe
- description: A pipeline parallel scheduling scheme with duplicated shards which has a very small pipeline bubble and allows perfect overlapping of communication and compute.
- impact on efficiency: 20/134 SMs used for comms, <calculate bubble>
- impact on model: None

- name: Loss free load balancing of experts
- description: In the top-K operation used to select experts, add a bias which reflects the load of that expert in the previous batches. This bias term evolves during training but is not trained by the optimiser and does not go into the calculation of the expert output.
- impact on efficiency: keeps even loads across experts and makes expert parallelism efficient.
- impact on model: ~1.5% benchmark improvement, experts specialise on topics more.


<!--
- name: DeepSeekMoE
- description: A "fine-grained" mixture of expert: for each token v3 chooses 8 experts among 256, and adds 1 shared expert through which all tokens are processed. These MoE layers replace the Feed-Forward Networks in all but the first 3 layers.
- impact on efficiency: 1/10 th FLOPs per token.
- impact on model performance: unknown - would need an ablation on the same dataset with a dense transformer.
-->

The improvements described here do not amount to the "order of magnitude performance improvement" that caused a stock market panic. So where is this performance coming from? Is it hidden or fake?
No, it's the natural consequence of successfully scaling up their "fine-grained" Mixture of Expert and Multi-head Latent Attention (MLA) that DeepSeek published in their DeepSeek-LLM and DeepSeek-V2 papers in early and mid-2024.

We will unpack how all those innovations work and stack-up against the other heavy-weight of open models, Llama3-405B, includ

### Model performance

<!-- R1-shaped hole for your consideration-->

### Takeaways

This success does not come out of nowhere!
It is the logical continuation of the work that DeepSeek has published throughout 2024. They have been vying with Meta's Llama family of models for the best open weight model for a year.
The efficiency of the training pipeline is a superb achievement of engineering, and it is fantastic to have another organisation publish what works at scale and what is needed to push the frontier.
