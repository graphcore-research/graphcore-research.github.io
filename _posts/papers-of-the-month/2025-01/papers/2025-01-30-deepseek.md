---
title: "DeepSeek-V3 & DeepSeek-R1 Technical Reports"
paper_authors: "DeepSeek-AI"
orgs: "DeepSeek-AI"
paper_link: "https://github.com/deepseek-ai/DeepSeek-V3"
tags:
    - efficient-inference
    - quantisation  # Use https://graphcore-research.github.io/tags/ as reference
potm_year: 2025
potm_month: 01
paper_order: 1  # Editor will decide
image_dir: "/assets/images/posts2025-01/potm/deepseek/"
review_author:
    name: "Alexandre Payot"
    link: "https://www.linkedin.com/in/alexandre-payot/"

hidden: true
---

With their V3 and R1 models, DeepSeek sets a new state-of-the-art in open-weight models and trades benchmark to benchmark with the best models from Anthropic, Google and OpenAI.
The technical reports give detailed accounts of the architecture of the model, the trade-offs that led to it, and the efficient implementation that enabled their final training run to take a headline-grabbing $5.5M of GPU hours.

<!-- Luke - feel free to use this in your editor's blurb at the top -->
DeepSeek's double release is so packed with information that we don't have space to cover their contributions in detail in this issue of "Papers of the Month"!
Instead, to strike a compromise, today, we give you short, clear descriptions of innovations and their impact on efficiency and model performance; and we will be back shortly with a deep-dive into DeepSeek's publications and how their releases compare to other labs'.

### The key idea

DeepSeek's success comes from improvements along three complementary axes: FLOP efficiency of the model, curation of training data, and (re-)discovery of effective reinforcement learning for LLMs.

DeepSeek's efficiency is the result of excellent engineering and incremental improvements to their fine-grained mixture of expert (MoE) architecture. Their innovations are geared towards making that architecture run efficiently on their hardware and learn effectively.

<!-- R1-key idea shaped hole -->

While the reports are thorough, some elements are notable by their absence: the team does not share scaling laws (as done in the ["Llama 3 herd of models" report](https://arxiv.org/abs/2407.21783)), and is unspecific about the dataset curation (as in the [Phi-4 technical report](https://arxiv.org/abs/2412.08905)).

### Efficiency improvements

In the table below we cover the architectural innovations, implementation optimizations, and the ablations that are in the DeepSeek-V3 technical report.
V3 is a 671 billion parameter "fine-grained" Mixture of Expert (MoE); it uses 8 routed experts per token out of 256 available, with 1 shared expert, it uses multi-head latent attention (MLA) with 128 heads. It was trained on 2048 Nvidia H800 GPUs in about 2 months.

<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Description</th>
      <th>Impact on Efficiency</th>
      <th>Impact on Model</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>
      <b>Mixed precision FP8 micro-scaled training</b>
      </td>
      <td>
      Does matrix multiplication with FP8 tensors scaled in blocks (N<sub>c</sub> Ã— N<sub>c</sub>) or vectors (N<sub>c</sub>), very similar to the micro-scaling (MX) formats available in Blackwell GPUs. The "trick" is to accumulate inside the blocks of 128 in FP8 and apply the scale and accumulate between blocks in FP32. The authors share a huge amount of interesting detail on this topic!
      </td>
      <td>
      Theoretically: 2x FLOPs available, 1/2 communication, 1/2 memory usage (capacity and BW).
      If memory was the limit, this might be what enabled them to train such a big model.
      </td>
      <td>
      Remarkably, None. Loss curves in the Appendix show it, but it took a lot of work to get there.
      </td>
    </tr>
    <tr>
      <td>
      <b>Multi-token Prediction</b>
      </td>
      <td>
      The MTP module is made of a transformer block combined with the main model's embeddings and output layers. It is trained with the main model to predict an extra token from the last transformer layer's output and the input sequence shifted one to the right.
      </td>
      <td>
      In training: ~2% increase in compute. In inference: x1.8 increase in throughput using the module as a speculative decoder
      (predicts 1 extra token per forward pass with 85% accuracy).
      </td>
      <td>
      Ablations show an average 2% improvement across benchmarks (extremes: -1%, +8%).
      </td>
    </tr>
    <tr>
      <td>
      <b>DualPipe</b>
      </td>
      <td>
      A pipeline parallel scheduling scheme with duplicated shards which has a very small pipeline bubble and allows perfect overlapping of communication and compute.
      </td>
      <td>
      Perfect computation/communication overlap... at the cost of
      15% of the GPU's compute used for communication (20/134 SMs).
      </td>
      <td>
      None
      </td>
    </tr>
    <tr>
      <td>
      <b>Loss free load balancing of experts</b>
      </td>
      <td>
      In the top-K operation used to select experts, add a bias which reflects the load of that expert in the previous batches. This bias term evolves during training but is not trained by the optimizer and does not go into the calculation of the expert output.
      </td>
      <td>
      Keeps even loads across experts and makes expert parallelism efficient. DeepSeek uses 64-way expert parallelism.
      </td>
      <td>
      Ablations show ~1.5% benchmark improvement, experts specialize more on discernible topics.
      </td>
    </tr>
  </tbody>
</table>


<!--
Removed will be for the deep-dive.
- name: DeepSeekMoE
- **description:** A "fine-grained" mixture of expert: for each token v3 chooses 8 experts among 256, and adds 1 shared expert through which all tokens are processed. These MoE layers replace the Feed-Forward Networks in all but the first 3 layers.
- **impact on efficiency:** 1/10 th FLOPs per token.
- impact on model performance: unknown - would need an ablation on the same dataset with a dense transformer.
-->

The improvements described here do not amount to the "order of magnitude performance improvement" that caused a stock market panic. So where is this performance coming from? Is it hidden or fake?

No! It's the natural consequence of successfully scaling up DeepSeek's "fine-grained" Mixture of Expert and Multi-head Latent Attention (MLA) that the DeepSeek-LLM and DeepSeek-V2 papers shared in early and mid-2024.
We will unpack how all those innovations work and stack up against the other heavyweight of open models, Llama3-405B, in a follow-up blog post.

### Model performance

<!-- R1-shaped hole for your consideration-->

### Takeaways

This success does not come out of nowhere!
It is the logical continuation of the work that DeepSeek has published throughout 2024. They have been vying with Meta's Llama family of models for the best open weight model for a year.
The efficiency of the training pipeline is a superb achievement of engineering, and it is fantastic to have another organization publish what works at scale and what is needed to push the frontier.