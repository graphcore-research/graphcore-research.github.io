---
title: "Fluid: Scaling Autoregressive Text-to-image Generative Models with Continuous Tokens"
paper_authors: "Lijie Fan, et al."
orgs: "Google DeepMind"
paper_link: "https://arxiv.org/abs/2410.13863"
tags:
    - image-generation
    - computer-vision
    - transformers  # Use https://graphcore-research.github.io/tags/ as reference
potm_year: 2024
potm_month: 10
paper_order: 1  # Editor will decide
image_dir: "/assets/images/posts/2024-10/potm/fluid/"
review_author:
    name: "Luka Ribar"
    link: "https://www.linkedin.com/in/luka-ribar/"
hidden: true
---

With the success of models such as Stable Diffusion, DALL-E 3, and Imagen, diffusion models have become the de facto standard for generative text-to-image models. At the same time, unlike their dominance in natural language processing, autoregressive models have been lagging behind. In order to narrow the gap between the two, the authors of this paper study the usage of continuous tokens, as well as the generation order of the image tokens. By training models up to 10.5B size, they show that a random-order autoregressive model using continuous tokens showcases improved performance.

### The key idea

A few sentences outlining why the paper is interesting...

Add images where appropriate throughout. This section should always
have at least 1 key figure though.

*Please use high-res images (zoom in for those screenshots!)*

<img src="{{ page.image_dir | append: 'figure_1.png' | relative_url }}" alt="A specific and succinct sentence or two describing the figure (alt text). Valuable for seo and accessibility.">
<figcaption>Figure 1a. If the caption isn't included in the image, it should be added like so.</figcaption>

### [optional] Background

If necessary, a short intro to background matierial needed to understand the method

### Their method

Latex can be included in the standard way, either inline: $R=\sum _{t=0}^{\infty }\gamma ^{t}r_{t}$

Or as a block:

<div>
$$
Q_{t+1}^{A}(s_{t},a_{t})=Q_{t}^{A}(s_{t},a_{t})+\alpha _{t}(s_{t},a_{t})\left(r_{t}+\gamma Q_{t}^{B}\left(s_{t+1},\mathop {\operatorname {arg~max} } _{a}Q_{t}^{A}(s_{t+1},a)\right)-Q_{t}^{A}(s_{t},a_{t})\right).
$$
</div>

Code can also be included in the standard way:

```
import popart

builder = popart.Builder()

# Build a simple graph
i1 = builder.addInputTensor(popart.TensorInfo("FLOAT", [1, 2, 32, 32]))
i2 = builder.addInputTensor(popart.TensorInfo("FLOAT", [1, 2, 32, 32]))

o = builder.aiOnnx.add([i1, i2])
```

### Results

...

### [optional] Takeaways

...
