---
title: "ParetoQ: Scaling Laws in Extremely Low-bit LLM Quantization"
paper_authors: "Zechun Liu, et al."
orgs: "Meta"
paper_link: "https://arxiv.org/abs/2502.02631"
tags:
    - efficient-inference
    - LLMs
    - quantisation
    - scaling-laws
potm_year: 2025
potm_month: 02
paper_order: 2
image_dir: "/assets/images/posts/2025-02/potm/paretoq/"
review_author:
    name: "Paul Balanca"
    link: "https://github.com/balancap"
hidden: true
---


### The key idea

Quantization is a key ingredient for efficient and cheap Large Language Model (LLM) servicing. As numerous quantization recipes have been published over the last couple of years, researchers and practitioners have felt a growing need for an experimental setting comparing all techniques. ParetoQ introduces the first unified framework for comparing different bit-width quantization approaches (1-bit, 1.58-bit, 2-bit, 3-bit, and 4-bit) for LLMs, with a comprehensive analysis considering five key dimensions: model parameters, token count, bit precision, training scheme and quantization function. ParetoQ reveals a critical learning transition between 2-bit and 3-bit quantization, where models quantized to 3-bit and above remain close to their full-precision pre-trained distributions, while lower-bit models require more substantial representation changes. 

### Background

Quantization is crucial for deploying LLMs efficiently, as it reduces memory requirements and computational costs. Previous research has yielded contradictory conclusions about optimal bit-width – some arguing for 4-bit, others for 1.58-bit quantization – and training setup (from quantization-aware training from scratch to simple post-training quantization). These inconsistencies emerge because prior studies have not systematically compared different bit-widths with the same training procedures and quantization functions. 

### Method

ParetoQ introduces several methodological improvements in quantization aware training (QAT), establishing new guidelines:

* Optimal training budget allocation: the authors found that allocating ~90% of the training tokens to full-precision pre-training and ~10% to quantization-aware fine-tuning achieves the best results across bit-widths. Additionally, lower bit quantization (1-bit, 1.58-bit, 2-bit) requires more fine-tuning tokens and exhibits "reconstruction" behaviour (i.e., the model needs to form new semantic representations to maintain performance), while higher bit quantization (3-bit, 4-bit) reaches saturation faster and shows "compensation" behaviour (i.e. remaining close to their pre-trained distribution).

* Bit-specific quantization functions: different bit-widths require dedicated quantization approaches. The researchers developed Stretched Elastic Quant (SEQ) for 1.58-bit and 2-bit quantization, for a better balance of output levels while maintaining an even quantization of the full-precision weight span. For 3-bit and 4-bit quantization, the paper shows that including zero in the quantization grid is always beneficial.

<img src="{{ page.image_dir | append: 'paretoq-quantization-grid.png' | relative_url }}" alt="ParetoQ quantization grids.">

* In all bit-width quantization settings, it is shown that a learnable range outperforms statistics-based methods (e.g., min-max, quantiles, etc.)
due to its flexibility in optimizing range parameters with respect to the final loss. The gradient of the scale parameter can be estimated on the backward pass using a straight-through estimator.

### Results

<img src="{{ page.image_dir | append: 'paretoq-comparison.png' | relative_url }}" alt="Comparison to other quantization methods.">
<figcaption>Comparison of ParetoQ with previous SOTA quantization methods.</figcaption>

The ParetoQ framework achieved state-of-the-art results across all bit-width settings, outperforming the existing literature on specific bit-widths. As presented in the comparison table, it presents a major step forward in terms of accuracy for 2-bit and 3-bit quantization. The pareto optimal curve is showing the 2-bit and 3-bit quantization is now an accurate alternative to the more common 4-bit quantization solution, achieving similar accuracy with improved memory usage. 2-bit quantization is a particularly promising solution considering hardware constraints for efficient memory packing/unpacking and dot-product implementation.

<img src="{{ page.image_dir | append: 'paretoq-results.png' | relative_url }}" alt="ParetoQ results on MobileLLM and Llama models.">
