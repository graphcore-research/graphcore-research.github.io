---
title: "February Papers: TBD"
header:
    teaser: /assets/images/posts/2025-02/potm/twitter_card.png
    image: /assets/images/posts/2025-02/potm/twitter_card.png
    og_image: /assets/images/posts/2025-02/potm/twitter_card.png

date: 2025-02-27T01:00:00-00:00
potm_year: 2025
potm_month: 2

layout: paper-summaries-layout
category: "papers-of-the-month"
toc: true
toc_sticky: true
toc_label: "Papers"
toc_icon: "book"
author.twitter: "GCResearchTeam"
---


The first paper, **Distillation Scaling Laws**, presents a thorough study of distillation for Language Models, with the aim of estimating how student performance scales as a function of model size and amount of distillation data used -- offering very useful insights, in an era where distillation pretraining of LLMs is becoming more and more widespread to improve "capability per watt".

The problem of computational efficiency and inference cost reduction is also at the heart of **ParetoQ**, a new unified framework to optimize the trade-off between quantized model size and accuracy in extremely low-bit regimes. And if you are a quantization geek like we are, make sure to also read our summary of **Matryoshka Quantization**, DeepMind's solution for training a quantized model that can then be easily served at different lower numerical precisions, by leveraging the nested structure of integer data types.

Finally, we jump from pretraining scaling laws to **scaling up test-time compute**, with a paper that introduces a recurrent block in the transformer architecture to allow the model to perform iterative reasoning in latent space at test-time to improve its performance.

*We hope you enjoy these month's papers as much as we did! If you have thoughts or questions, please reach out to us at [@GCResearchTeam](https://x.com/GCResearchTeam).*

---

{% include paper-summaries.md %}