---
title: "June Papers: Gradient norms, LLM reasoning, and video generation"
header:
    teaser: /assets/images/posts/2025-06/potm/twitter_card.png
    image: /assets/images/posts/2025-06/potm/twitter_card.png
    og_image: /assets/images/posts/2025-06/potm/twitter_card.png

date: 2025-07-01T01:00:00-00:00
potm_year: 2025
potm_month: 6

layout: paper-summaries-layout
category: "papers-of-the-month"
toc: true
toc_sticky: true
toc_label: "Papers"
toc_icon: "book"
author.twitter: "GCResearchTeam"
---

This June not only brought us very hot and sunny days (at least here in the UK), but also an excellent selection of new and exciting ML research! Out of the many good candidates, this month we selected three papers, covering quite a lot of different ground.

In the first paper, [Why Gradients Rapidly Increase Near the End of Training](#why-gradients-rapidly-increase-near-the-end-of-training), a researcher from FAIR explores the puzzling phenomenon of increasing gradient magnitudes during training, offering an elegant mathematical explanation and a simple remedy.

Next, in [ProRL](#prorl-prolonged-reinforcement-learning-expands-reasoning-boundaries-in-large-language-models), NVIDIA researchers dive into the evolving topic of large language model reasoning, showing how prolonged reinforcement learning can indeed introduce novel reasoning abilities.

Finally, we look at [AAPT](#autoregressive-adversarial-post-training-for-real-time-interactive-video-generation), a fresh approach from the ByteDance Seed team that turns pre-trained offline diffusion models into real-time video generators via adversarial post-training.

*We hope you enjoy this month's papers as much as we did! If you have thoughts or questions, please reach out to us at [@GCResearchTeam](https://x.com/GCResearchTeam).*

---

{% include paper-summaries.md %}
